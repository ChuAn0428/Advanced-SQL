{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010612,
     "end_time": "2020-10-01T00:24:15.090493",
     "exception": false,
     "start_time": "2020-10-01T00:24:15.079881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Sometimes it doesn't matter whether your query is efficient or not. For example, you might write a query you expect to run only once, and it might be working on a small dataset. In this case, anything that gives you the answer you need will do.\n",
    "\n",
    "But what about queries that will be run many times, like a query that feeds data to a website? Those need to be efficient so you don't leave users waiting for your website to load.\n",
    "\n",
    "Or what about queries on huge datasets? These can be slow and cost a business a lot of money if they are written poorly.\n",
    "\n",
    "Most database systems have a **query optimizer** that attempts to interpret/execute your query in the most effective way possible. But several strategies can still yield huge savings in many cases.\n",
    "\n",
    "# Some useful functions\n",
    "\n",
    "We will use two functions to compare the efficiency of different queries:\n",
    "- `show_amount_of_data_scanned()` shows the amount of data the query uses.\n",
    "- `show_time_to_run()` prints how long it takes for the query to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-01T00:24:15.119251Z",
     "iopub.status.busy": "2020-10-01T00:24:15.117118Z",
     "iopub.status.idle": "2020-10-01T00:24:15.124878Z",
     "shell.execute_reply": "2020-10-01T00:24:15.125496Z"
    },
    "papermill": {
     "duration": 0.025809,
     "end_time": "2020-10-01T00:24:15.125676",
     "exception": false,
     "start_time": "2020-10-01T00:24:15.099867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Kaggle's public dataset BigQuery integration.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.cloud import bigquery\n",
    "from time import time\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "def show_amount_of_data_scanned(query):\n",
    "    # dry_run lets us see how much data the query uses without running it\n",
    "    dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
    "    query_job = client.query(query, job_config=dry_run_config)\n",
    "    print('Data processed: {} GB'.format(round(query_job.total_bytes_processed / 10**9, 3)))\n",
    "    \n",
    "def show_time_to_run(query):\n",
    "    time_config = bigquery.QueryJobConfig(use_query_cache=False)\n",
    "    start = time()\n",
    "    query_result = client.query(query, job_config=time_config).result()\n",
    "    end = time()\n",
    "    print('Time to run: {} seconds'.format(round(end-start, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009131,
     "end_time": "2020-10-01T00:24:15.144632",
     "exception": false,
     "start_time": "2020-10-01T00:24:15.135501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Strategies\n",
    "\n",
    "### 1) Only select the columns you want. \n",
    "\n",
    "It is tempting to start queries with **SELECT * FROM ...**. It's convenient because you don't need to think about which columns you need. But it can be very inefficient.\n",
    "\n",
    "This is especially important if there are text fields that you don't need, because text fields tend to be larger than other fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:24:15.170970Z",
     "iopub.status.busy": "2020-10-01T00:24:15.170100Z",
     "iopub.status.idle": "2020-10-01T00:24:16.465884Z",
     "shell.execute_reply": "2020-10-01T00:24:16.464790Z"
    },
    "papermill": {
     "duration": 1.311815,
     "end_time": "2020-10-01T00:24:16.466044",
     "exception": false,
     "start_time": "2020-10-01T00:24:15.154229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed: 2523.552 GB\n",
      "Data processed: 2.412 GB\n"
     ]
    }
   ],
   "source": [
    "star_query = \"SELECT * FROM `bigquery-public-data.github_repos.contents`\"\n",
    "show_amount_of_data_scanned(star_query)\n",
    "\n",
    "basic_query = \"SELECT size, binary FROM `bigquery-public-data.github_repos.contents`\"\n",
    "show_amount_of_data_scanned(basic_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010155,
     "end_time": "2020-10-01T00:24:16.487233",
     "exception": false,
     "start_time": "2020-10-01T00:24:16.477078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this case, we see a 1000X reduction in data being scanned to complete the query, because the raw data contained a text field that was 1000X larger than the fields we might need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010062,
     "end_time": "2020-10-01T00:24:16.507683",
     "exception": false,
     "start_time": "2020-10-01T00:24:16.497621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2) Read less data.\n",
    "\n",
    "Both queries below calculate the average duration (in seconds) of one-way bike trips in the city of San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:24:16.537994Z",
     "iopub.status.busy": "2020-10-01T00:24:16.537133Z",
     "iopub.status.idle": "2020-10-01T00:24:17.378262Z",
     "shell.execute_reply": "2020-10-01T00:24:17.377382Z"
    },
    "papermill": {
     "duration": 0.860317,
     "end_time": "2020-10-01T00:24:17.378419",
     "exception": false,
     "start_time": "2020-10-01T00:24:16.518102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed: 0.076 GB\n",
      "Data processed: 0.06 GB\n"
     ]
    }
   ],
   "source": [
    "more_data_query = \"\"\"\n",
    "                  SELECT MIN(start_station_name) AS start_station_name,\n",
    "                      MIN(end_station_name) AS end_station_name,\n",
    "                      AVG(duration_sec) AS avg_duration_sec\n",
    "                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n",
    "                  WHERE start_station_id != end_station_id \n",
    "                  GROUP BY start_station_id, end_station_id\n",
    "                  LIMIT 10\n",
    "                  \"\"\"\n",
    "show_amount_of_data_scanned(more_data_query)\n",
    "\n",
    "less_data_query = \"\"\"\n",
    "                  SELECT start_station_name,\n",
    "                      end_station_name,\n",
    "                      AVG(duration_sec) AS avg_duration_sec                  \n",
    "                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n",
    "                  WHERE start_station_name != end_station_name\n",
    "                  GROUP BY start_station_name, end_station_name\n",
    "                  LIMIT 10\n",
    "                  \"\"\"\n",
    "show_amount_of_data_scanned(less_data_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011244,
     "end_time": "2020-10-01T00:24:17.403312",
     "exception": false,
     "start_time": "2020-10-01T00:24:17.392068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since there is a 1:1 relationship between the station ID and the station name, we don't need to use the `start_station_id` and `end_station_id` columns in the query.  By using only the columns with the station IDs, we scan less data.\n",
    "\n",
    "### 3) Avoid N:N JOINs.\n",
    "\n",
    "Most of the JOINs that you have executed in this course have been **1:1 JOINs**.  In this case, each row in each table has at most one match in the other table.\n",
    "\n",
    "![JOIN](https://i.imgur.com/fp7oMLq.png)\n",
    "\n",
    "Another type of JOIN is an **N:1 JOIN**.  Here, each row in one table matches potentially many rows in the other table.  \n",
    "\n",
    "![JOIN](https://i.imgur.com/7PxE0Mr.png)\n",
    "\n",
    "Finally, an **N:N JOIN** is one where a group of rows in one table can match a group of rows in the other table. Note that in general, all other things equal, this type of JOIN produces a table with many more rows than either of the two (original) tables that are being JOINed.\n",
    "\n",
    "![JOIN](https://i.imgur.com/UsNZZoz.png)\n",
    "\n",
    "Now we'll work with an example from a real dataset.  Both examples below count the number of distinct committers and the number of files in several GitHub repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:24:17.436105Z",
     "iopub.status.busy": "2020-10-01T00:24:17.435252Z",
     "iopub.status.idle": "2020-10-01T00:45:34.786630Z",
     "shell.execute_reply": "2020-10-01T00:45:34.785924Z"
    },
    "papermill": {
     "duration": 1277.371761,
     "end_time": "2020-10-01T00:45:34.786759",
     "exception": false,
     "start_time": "2020-10-01T00:24:17.414998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 1272.855 seconds\n",
      "Time to run: 4.494 seconds\n"
     ]
    }
   ],
   "source": [
    "big_join_query = \"\"\"\n",
    "                 SELECT repo,\n",
    "                     COUNT(DISTINCT c.committer.name) as num_committers,\n",
    "                     COUNT(DISTINCT f.id) AS num_files\n",
    "                 FROM `bigquery-public-data.github_repos.commits` AS c,\n",
    "                     UNNEST(c.repo_name) AS repo\n",
    "                 INNER JOIN `bigquery-public-data.github_repos.files` AS f\n",
    "                     ON f.repo_name = repo\n",
    "                 WHERE f.repo_name IN ( 'tensorflow/tensorflow', 'facebook/react', 'twbs/bootstrap', 'apple/swift', 'Microsoft/vscode', 'torvalds/linux')\n",
    "                 GROUP BY repo\n",
    "                 ORDER BY repo\n",
    "                 \"\"\"\n",
    "show_time_to_run(big_join_query)\n",
    "\n",
    "small_join_query = \"\"\"\n",
    "                   WITH commits AS\n",
    "                   (\n",
    "                   SELECT COUNT(DISTINCT committer.name) AS num_committers, repo\n",
    "                   FROM `bigquery-public-data.github_repos.commits`,\n",
    "                       UNNEST(repo_name) as repo\n",
    "                   WHERE repo IN ( 'tensorflow/tensorflow', 'facebook/react', 'twbs/bootstrap', 'apple/swift', 'Microsoft/vscode', 'torvalds/linux')\n",
    "                   GROUP BY repo\n",
    "                   ),\n",
    "                   files AS \n",
    "                   (\n",
    "                   SELECT COUNT(DISTINCT id) AS num_files, repo_name as repo\n",
    "                   FROM `bigquery-public-data.github_repos.files`\n",
    "                   WHERE repo_name IN ( 'tensorflow/tensorflow', 'facebook/react', 'twbs/bootstrap', 'apple/swift', 'Microsoft/vscode', 'torvalds/linux')\n",
    "                   GROUP BY repo\n",
    "                   )\n",
    "                   SELECT commits.repo, commits.num_committers, files.num_files\n",
    "                   FROM commits \n",
    "                   INNER JOIN files\n",
    "                       ON commits.repo = files.repo\n",
    "                   ORDER BY repo\n",
    "                   \"\"\"\n",
    "\n",
    "show_time_to_run(small_join_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012448,
     "end_time": "2020-10-01T00:45:34.811300",
     "exception": false,
     "start_time": "2020-10-01T00:45:34.798852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The first query has a large N:N JOIN.  By rewriting the query to decrease the size of the JOIN, we see it runs much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011582,
     "end_time": "2020-10-01T00:45:34.834840",
     "exception": false,
     "start_time": "2020-10-01T00:45:34.823258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Learn more\n",
    "\n",
    "These strategies and many more are discussed in [this thorough guide](https://www.oreilly.com/library/view/google-bigquery-the/9781492044451/) to Google BigQuery.  If you'd like to learn more about how to write more efficient queries (or deepen your knowledge of all things BigQuery), you're encouraged to check it out!\n",
    "\n",
    "# Your turn\n",
    "Leverage what you've learned to **[improve the design](https://www.kaggle.com/kernels/fork/5045822)** of several queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011724,
     "end_time": "2020-10-01T00:45:34.858535",
     "exception": false,
     "start_time": "2020-10-01T00:45:34.846811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/161315) to chat with other Learners.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1284.814577,
   "end_time": "2020-10-01T00:45:34.978754",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-01T00:24:10.164177",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
